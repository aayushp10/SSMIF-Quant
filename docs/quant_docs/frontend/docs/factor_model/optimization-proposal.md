---
id: optimization-proposal
title: Optimizaton Proposal
sidebar_label: Optimization Proposal
slug: /optimization-proposal
---
### Developers
- [Michael DiGregorio](https://www.linkedin.com/in/michael-jonathan-digregorio/)

## How this works

### Training & Testing Data

- Have training and testing data split with k-folds, preserving time-series data and randomizing permutations of input data
- This allows for truly random training and testing data split, and an increase in the amount of training data while time continuity is still preserved.
- The data used will be compounded continuous returns (CCR), eliminating the issue of price discontinuity between training batches. 
- For shorter windows, we will use 5 min trading data
- For longer windows, we will use daily returns

### "Submodels"

- There are several different models that are being used to create a holistic factor model. Currently we use linear regression, random forest, and ARIMA regression. The window size is a variable factor, with lower window sizes showing volatility and higher window sizes showing macro trends. As such the submodels should include both short and long-term window sizes, with our final output being generated from a linear combination of the outputs of each submodel (Linear Regression, ARIMA Regression, Random Forest).

### Meta Labeling

- Use weightings to aggregate the outputs of the machine learning models into one final result. We will currently feed the model testing errors into a softmax function to generate the weights assigned each model, with regards to their influence on our final CCR prediction.

## Example

- Assume a testing dataset broken up into 20 periods:
  - [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
- A long window size testing window of 4
- A long window size gap of 2
- A short window size testing window of 2
- A short window size gap of 2

The data will be labeled as such: (train / gap / test)

### Long Window

- [(1,2,3,4 / 5,6 / 7,8,9,10), (11,12,13,14 / 15,16 / 17,18,19,20)]

### Short Window

- [(1,2 / 3 / 4,5), (6,7 / 8 / 9,10), (11,12 / 13 / 14,15), (16,17 / 18 / 19,20)]

- Importantly, these segments of data do not have to be contiguous, only the (intra-fold) data needs to be contiguous. This means that we can train models to match certain regimes or specific market conditions.
- Each intra-fold data subset will be of the same length. It is passed in through batches one at a time and used to train the model. Though each batch is not continuous with other subsets, the model is able to train on each fold, which are internally continuous.

## How to Backtest Our Black-Litterman

- Run the model on any arbitrary timeframe, and find the optimal assets to purchase. Then compare the performance of the assets picked to the S&P500 returns.
- Since we will be optimizing using a modified Black Litterman model, we will backtest as so: 
  - Our modified Black Litterman allows us to do two things that others cannot do with a prebuilt model:
    - We can modify our optimization "cost" function to change the ratio of the volatility penalty to mean returns penalty - we can also change our model to only penalize downwardFs volatility. i.e we can choose to optimize for Sortino Ratio or Sharpe Ratio: 
    - The cost function would look like this: variance + (-0.1 * (mean - r)), where
      - mean: actual returns of our portfolio
      - r: target returns of our portfolio
      - variance: variance of our portfolio
      - -0.1: Mean Penalty Constant (MPC)
    - If the MPC of our model is negative and our portfolio returns fall below our target, our penalty increases. Conversely, if we exceed our target returns our penalty is reduced (Sortino optimization)
    - If the MPC of our model is positive and our portfolio returns fall below our target our penalty increases, and if our portfolio returns exceed our target we would penalize that just as much. We penalize upwards and downwards volatility equivalently (Sharpe optimizaton)
    - The second thing is that we don't need a "human input" ðŸš€ to the model to generate our "investor view" on asset returns. This "investor view" of the market's future returns is generated by our return prediction models from earlier in the pipeline.
  - Because our "human input" is computer generated based off of prior market data, we can effectively backtest the model without approximating what humans would have felt about the market at some point in the past

## Why is this better than the previous factor model

- Assume the investment window is 5 years
- Assume we have access to data from 2010 - 2020 and that it is currently 2020
- The old factor model would train on the data from 2010 - 2015.
- It would then use the training errors obtained from prediction on the 2010 - 2015 data to generate a "return prediction" from the outputs of the three submodels (Linear regression, ARIMA, Random Forest)
- These model parameters would then be used to comprise the "trained model" 
- The trained model would be fed data from 2015 - 2020 and asked to make a price prediction for those years
- A set of several hundred thousand randomly allocated portfolios are generated starting in 2015 and tested over the data from 2015-2020
- The best performing portfolio is chosen as our alloation for the current week under the assumption that "if it worked for the past 5 years, it will work for the next couple too"

- This is not a very reactive method for generating portfolio allocations because it trains to a "mean" This means that we are unable to react quickly to black swan events and regume changes like we saw in 2020. That data will be a small minority of the data tested on, and not trained on at all under the old framework.

- Under the new framework we can both train and test off of that data, and all segments of data like it, going as far back as we have access to data for. This makes our model more reactive and more able to optimize for specific market conditions
